{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Think Digital Summit 2020 Code Cafe RL experince\n",
    "\n",
    "Presented by [Nextgrid](https://nextgrid.ai)\n",
    "\n",
    "<img align=\"left\" width=\"100%\" height=\"auto\" src=\"https://nextgrid.ai/wp-content/uploads/2020/08/ibm-think-2020.jpg\"/>\n",
    "\n",
    "<div style=\"max-width:700px;\">\n",
    "\n",
    "## Reinforcment Learning Hands-on\n",
    "\n",
    "Reinforcement learning is a machine learning technique that learns how to maximize a reward by taking actions. A dog might try to learn how to maximize belly rubs through its barking, or a cat might try to learn how to maximize being annoying through its jumping. Both these animals are **AGENTS** taking **ACTIONS** based on their current **STATE**, trying to maximize the **REWARD**.  \n",
    "### Goal = maximize reward    \n",
    "    \n",
    "The goal of the **AGENT** is to **maximize its total reward**. It does this by adding the maximum reward attainable from future states to the reward for achieving its current state, effectively influencing the current action by the potential future reward. This potential reward is a weighted sum of the expected values of the rewards of all future steps starting from the current state.\n",
    "\n",
    "<img align=\"left\" width=\"500\" hight=\"auto\" src=\"https://nextgrid.ai/wp-content/uploads/2020/08/RL.png\"></img>\n",
    "\n",
    "<div style=\"clear:left;\"></div>\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Todays hands-on task - Gym, CartPole v1\n",
    "---\n",
    "\n",
    "Today we will use be using a **Reinforcment Learning Algoritm** on **IBM Watson studio** to **train** an **agent** on how to **efficiently** balance a pole on a cart\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img align=\"left\"  src=\"https://nextgrid.ai/wp-content/uploads/2020/09/cartpole.gif\"/><div style=\"clear:left;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## CartPole Environment\n",
    "\n",
    "\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "The **pendulum starts upright, and the goal is to prevent it from falling over**. \n",
    "A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "<br/>\n",
    "\n",
    "### Observation\n",
    "\n",
    "Type: Box(4)  \n",
    "\n",
    "<img align=\"left\" width=\"400\" src=\"https://nextgrid.ai/wp-content/uploads/2020/09/Screenshot-2020-09-10-at-10.28.17-e1599728746594.png\"/><div style=\"clear:left;\"/>\n",
    "<br/>\n",
    "\n",
    "### Actions\n",
    "\n",
    "Type: Discrete(2)\n",
    "\n",
    "Action is two real values vector from -1 to +1 where [0] controls the throttle of the main engine and [1] controls the throttle of left & right boosters. Fuel is infinite, so an agent can learn to fly and then land on its first attempt\n",
    "\n",
    "<img align=\"left\" width=\"400\" src=\"https://nextgrid.ai/wp-content/uploads/2020/09/Screenshot-2020-09-10-at-10.28.22.png\"/><div style=\"clear:left;\"/>\n",
    "<br/>   \n",
    "   \n",
    "   \n",
    "   \n",
    "\n",
    "### Reward\n",
    "\n",
    "A reward of +1 is provided for every timestep that the pole remains upright.\n",
    "\n",
    "### Episode Termination\n",
    "1. Pole Angle is more than ±12°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 500\n",
    "\n",
    "### Solved Requirements\n",
    "Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "\n",
    "# Libraries\n",
    "---\n",
    "\n",
    "For this exersice we will be using following libraries and environments\n",
    "\n",
    "**[OpenAi Gym](https://gym.openai.com/)**  \n",
    "Toolkit for developing and comparing reinforcement learning algorithms. It supports teaching agents everything from walking to playing games like Pong or Pinball.\n",
    "\n",
    "**[Box2D](https://box2d.com)**  \n",
    "A 2D Physics Engine\n",
    "\n",
    "**[Stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/)**  \n",
    "Set of improved implementations of reinforcement learning algorithms based on OpenAI [Baselines](https://github.com/openai/baselines/).\n",
    "\n",
    "\n",
    "## Stable Baselines\n",
    "\n",
    "\n",
    "<img align=\"left\" width=\"200\" src=\"https://github.com/hill-a/stable-baselines/raw/master/docs//_static/img/logo.png\"/><div style=\"clear:left;\"/>\n",
    "Stable Baselines is a set of improved implementations of reinforcement learning algorithms based on OpenAI Baselines. Stable baselines 3 that we are using today is based upon [Pytorch](https://pytorch.org/).\n",
    "\n",
    "<img align=\"left\" width=\"500\" src=\"https://nextgrid.ai/wp-content/uploads/2020/09/Screenshot-2020-09-10-at-12.34.52-e1599734718305.png\"/><div style=\"clear:left;\"/>\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "# RL Algoritm - DQN Deep Q-network\n",
    "---\n",
    "In deep Q-learning, we use a **neural network** to approximate the Q-value function. The state is given as the input and the Q-value of all possible actions is generated as the output. The comparison between Q-learning & deep Q-learning is illustrated below:\n",
    "<img align=\"left\" width=\"1000\" src=\"https://nextgrid.ai/wp-content/uploads/2020/09/Screenshot-2020-09-10-at-13.05.08.png\"/><div style=\"clear:left;\"/>\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "\n",
    "### How to run cells\n",
    "\n",
    "- Option 1: Select the cell and click Run in menu\n",
    "- Option 2: Shift + Enter\n",
    "\n",
    "<img align=\"left\" width=\"1000\" src=\"https://nextgrid.ai/wp-content/uploads/2020/09/ezgif.com-crop.gif\"/><div style=\"clear:left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python libraries\n",
    "\n",
    "Start by installing required packages using Python package manager `pip` \n",
    "\n",
    "**We add following packages**\n",
    "```\n",
    "stable-baselines3 box2d box2d-kengz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install stable-baselines3 box2d box2d-kengz pyglet==1.5.0 cloudpickle==1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Python packages\n",
    "Import the Python packages that we will use, add packages to the code cell below\n",
    "\n",
    "#### Todo\n",
    "- Add stable_baseline3 packages to cell\n",
    "\n",
    "```\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import IPython\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "# Import stable_baselines packages\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook values and folders\n",
    "Set the values and folders that will used in notebook. \n",
    "\n",
    "#### Todo\n",
    "\n",
    "- Add `CartPole-v1` in ```env_id = ''``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviorment\n",
    "env_id = 'CartPole-v1'   \n",
    "\n",
    "# Set and make log folder\n",
    "log_dir = 'log'            \n",
    "os.makedirs(log_dir, exist_ok=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Simple example\n",
    "Let's start with a simple example where we configure the enviorment and train our model. We will also evaluate the models score before and after the training\n",
    "1. import RL algoritm & policy\n",
    "2. configure enviorment & hyperparmeters\n",
    "4. Evaluate before training\n",
    "3. Train model\n",
    "4. Evaluate after training\n",
    "5. Plotting\n",
    "6. Save model \n",
    "7. Delete model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import RL algoritm & policy \n",
    "From `stable_baselines` we import the `DQN` algoritm and `MlpPolicy`  \n",
    "`MlpPolicy` = Policy object that implements actor critic, using a MLP (2 layers of 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Configure enviorment\n",
    "- Configure our `env` using the values stated previously. \n",
    "- Instantiate the agent in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create environment\n",
    "env = gym.make(env_id)\n",
    "env = Monitor(env, log_dir)\n",
    "\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN(MlpPolicy, env, verbose=1)\n",
    "# model.set_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate before training model\n",
    "Before running model training we evaluate it by running 100 episodes and returning the score for each one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import evaluation helper from stable_baselines\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# evaluate the model by runnign it 100 times `n_eval_episodes=10` and then print the score for each round (episode) \n",
    "evals = evaluate_policy(model, env, n_eval_episodes=100, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=True)\n",
    "\n",
    "# print result\n",
    "print(evals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train model\n",
    "Train our model for 20000 steps using default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run model training\n",
    "model.learn(total_timesteps=75000, log_interval=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate after training model\n",
    "Lets evaluate the model again after training. \n",
    "Did the score improve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=True)\n",
    "print(evals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plotting\n",
    "We visualize the results with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import results plotter\n",
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "# Plot results\n",
    "results_plotter.plot_results([log_dir], 1e6, results_plotter.X_TIMESTEPS, \"DQN CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save model\n",
    "Save the current state of our agent by using `model.save()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save('my_cartpole_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls # check if my_cartpole_model(.zip) have been saved\n",
    "!cd log\n",
    "!ls log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Delete model\n",
    "We can reset our `model` by deleting the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model back again\n",
    "model = DQN.load(\"my_cartpole_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking it one step future\n",
    "You have now tried the basics of configuring your model, trained and evaluted it.  \n",
    "Now let's see how we can improve our result & training by configure hyperparameters and adding some automation\n",
    "\n",
    "### Delete model configuration\n",
    "Before continuing we delete the previous DQN model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete current model & configuration\n",
    "del model\n",
    "\n",
    "# delete log previous log files\n",
    "!rm -r ./log/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild enviorment \n",
    "env = gym.make(env_id)\n",
    "env = Monitor(env, log_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter configuration\n",
    "Configure the DQN algoritms hyperparameters.  \n",
    "Read more at https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\n",
    "    MlpPolicy,\n",
    "    env,\n",
    "    verbose=1,                         # display output when training, 0 = no output, 1 = show output\n",
    "    gamma=0.99,                        # discount for future rewards\n",
    "    learning_rate=0.00025,             # learning rate\n",
    "    buffer_size=100000,                # size of the replay buffer\n",
    "    batch_size=512,                    # number of transitions sampled from replay buffer\n",
    "    learning_starts=0,                 # info\n",
    "    target_update_interval=1250,       # info\n",
    "    train_freq=16,                     # steps before starting training\n",
    "    gradient_steps=4,                  # steps before starting training\n",
    "    exploration_fraction=0.06,         # steps before starting training\n",
    "    exploration_final_eps=0.11,        # steps before starting training\n",
    " \n",
    "    )\n",
    "\n",
    "# model.set_env(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model when new reward score is achieved\n",
    "Let's create a callback function that will evaluate our model every 25000 steps and save it if it performes better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq: (int)\n",
    "    :param log_dir: (str) Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose=1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}.zip\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the callback: check every 25000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=25000, log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=50000, log_interval=100, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting & Evaluating\n",
    "We use plotting & evaluation to messure the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evals = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True, render=False, callback=None, reward_threshold=None, return_episode_rewards=True)\n",
    "print(evals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common import results_plotter\n",
    "\n",
    "# Helper from the library\n",
    "results_plotter.plot_results([log_dir], 1e6, results_plotter.X_TIMESTEPS, \"DQN CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(values, window):\n",
    "    \"\"\"\n",
    "    Smooth values by doing a moving average\n",
    "    :param values: (numpy array)\n",
    "    :param window: (int)\n",
    "    :return: (numpy array)\n",
    "    \"\"\"\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    \"\"\"\n",
    "    plot the results\n",
    "\n",
    "    :param log_folder: (str) the save location of the results to plot\n",
    "    :param title: (str) the title of the task to plot\n",
    "    \"\"\"\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = moving_average(y, window=50)\n",
    "    # Truncate x\n",
    "    x = x[len(x) - len(y):]\n",
    "\n",
    "    fig = plt.figure(title)\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep training & evaluating\n",
    "Keep training and evaluating by adding new to empty code cells below to the notebook.   \n",
    "Read through the documentation linked above to understand how you can modify DQN to perform better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"1000\" src=\"https://nextgrid.ai/wp-content/uploads/2020/09/Screenshot-2020-09-10-at-12.53.12.png\"/><div style=\"clear:left;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
